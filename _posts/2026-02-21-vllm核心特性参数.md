---
title: vLLM核心特性参数和调优方向
date: 2026-02-21 10:00:00 +0800
categories: [LLM, Inference]
tags: [vllm, kv-cache]
pin: true
---

2025.10 之前总结的 vllm核心特性和优化方向。 

# 核心特性

## Speculative Decoding

```
1. Draft model 快速生成 k 个 candidate tokens
2. Target model 一次性验证所有 k 个 tokens
3. 接受匹配的 tokens, 拒绝不匹配的
4. 加速比取决于 acceptance rate

理想情况: k=5, 100% accept → 5x 加速
实际: 通常 2-3x 加速
```

核心思路：autoregressive decoding 每步只产一个 token，GPU 利用率低（decode 阶段是 memory-bound）。用一个小模型（draft model）快速猜 k 个 token，再让大模型（target model）**一次 forward pass** 并行验证，数学上等价于直接用大模型生成。

```
无 Speculative Decoding:
  Target: [t1] → [t2] → [t3] → [t4] → [t5]    5 次 forward pass

有 Speculative Decoding (k=5, 全部接受):
  Draft:   [t1, t2, t3, t4, t5]                  5 次小模型 forward
  Target:  [验证 t1~t5]                           1 次大模型 forward
  总耗时 ≈ 1 次大模型 + 5 次小模型 << 5 次大模型
```

适用场景：**延迟敏感 + 批量较小**。大 batch 时 decode 已经 compute-bound，加速效果减弱。

### 如何开启

```bash
# 方式一：独立的 draft model
vllm serve meta-llama/Llama-3.1-70B-Instruct \
    --speculative-model meta-llama/Llama-3.1-8B-Instruct \
    --num-speculative-tokens 5
```

`num_speculative_tokens` 越大，单次验证可能接受更多 token，但 acceptance rate 也会下降。通常 3~5 是比较好的平衡点。

## Prefix Caching

多个请求共享相同前缀（如 system prompt）时，缓存前缀的 KV Cache 避免重复计算。详细机制见 [Prefix Caching 对比](/posts/prefix-caching/)。

```
无 Prefix Caching:
  Req A: [system_prompt + query_A]  → 全量 prefill
  Req B: [system_prompt + query_B]  → 全量 prefill（system_prompt 重复计算）

有 Prefix Caching:
  Req A: [system_prompt + query_A]  → 全量 prefill, 缓存 system_prompt KV
  Req B: [system_prompt + query_B]  → 复用缓存, 只 prefill query_B 部分
```

适用场景：**Agent 多轮对话、RAG 场景**（大量请求共享长 system prompt）。system prompt 越长、请求量越大，收益越高。

### 如何开启

vLLM v0.6.0+ 默认开启 prefix caching（`enable_prefix_caching=True`）。

```bash
# 显式开启
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --enable-prefix-caching
```

## KV Cache 量化

将KV Cache 从 FP16 压缩到 FP8/INT8，**直接减半显存占用**，从而能容纳更多并发请求或更长上下文。

```
FP16 KV Cache 显存 (Llama-3.1-70B, 4K context, 1 req):
  = 2 (K+V) × 80 layers × 8 heads × 128 dim × 4096 tokens × 2 bytes
  ≈ 10 GB

FP8 KV Cache:
  = 同上 × 1 byte
  ≈ 5 GB      → 省出 5 GB 可以多跑请求
```

代价：精度有微小损失，对大多数任务影响可忽略。

### 如何开启

```bash
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --kv-cache-dtype fp8
```

KV Cache 量化与权重量化正交，可以同时使用。例如 AWQ 权重量化 + FP8 KV Cache 可以进一步压缩显存。

## Chunked Prefill

将长 prompt 的 prefill 拆成固定大小的 chunk，每个调度周期只处理一个 chunk，**剩余计算预算让给 decode 请求**。 本质是避免 占用 卡住其它reqs。

```
无 Chunked Prefill:
  ┌─────────────────────────────────────────────┐
  │  Prefill (8K tokens)                         │  ← 独占整个 batch，decode 全部等待
  └─────────────────────────────────────────────┘
  ┌──────┬──────┬──────┬──────┐
  │ Dec1 │ Dec2 │ Dec3 │ Dec4 │                    ← prefill 结束后才能 decode
  └──────┴──────┴──────┴──────┘

有 Chunked Prefill (chunk_size=2048):
  ┌───────────┬──────┬──────┬──────┐
  │ Prefill   │ Dec1 │ Dec2 │ Dec3 │  ← chunk 1 + decode 混合
  │ (2048 tok)│      │      │      │
  ├───────────┼──────┼──────┼──────┤
  │ Prefill   │ Dec1 │ Dec2 │ Dec3 │  ← chunk 2 + decode 混合
  │ (2048 tok)│      │      │      │
  ├───────────┼──────┼──────┼──────┤
  │ ...       │      │      │      │
  └───────────┴──────┴──────┴──────┘
```

核心收益：**降低已有请求的 TPOT（Time Per Output Token）波动**。没有它，一个超长 prompt 进来会阻塞所有 decode，造成延迟尖刺。

代价：长 prompt 的 TTFT 会略微增加（被分成多轮计算）。

### 如何开启

vLLM v0.6.0+ 默认开启。

```bash
# 控制每个调度周期最多处理多少 prefill tokens
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --max-num-batched-tokens 2048 \
    --enable-chunked-prefill
```

`max_num_batched_tokens` 就是 chunk size 的上限。值越小，decode 延迟越稳定，但 prefill 吞吐越低。通常 2048~8192 是合理范围。

## CUDA Graph

将 GPU kernel 调度从 CPU 逐个提交改为**预录制整个计算图，一次性回放**，消除 CPU launch overhead。

```
无 CUDA Graph:
  CPU: launch_kernel_1 → wait → launch_kernel_2 → wait → launch_kernel_3 → ...
  GPU: ████░░░░████░░░░████░░░░    (大量 idle gap)

有 CUDA Graph:
  CPU: replay_graph →    (一次调用)
  GPU: ████████████████████████    (紧密执行，无间隙)
```

Decode 阶段每步只处理少量 token，kernel 很小，CPU launch overhead 占比很高（可达 50%+），CUDA Graph 消除这部分开销。

代价：**graph capture 需要固定输入 shape**，vLLM 会为常见 batch_size 各录一份 graph，占用额外显存。

### 如何开启

默认开启，通过 `max_seq_len_to_capture` 控制范围。

```bash
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --max-seq-len-to-capture 8192   # 序列长度 ≤ 8192 时使用 CUDA Graph，超过则 fallback eager
    # --enforce-eager               # 完全关闭 CUDA Graph，用于调试
```

## Continuous Batching

传统 static batching 必须等整个 batch 中最慢的请求完成后才能释放资源。Continuous batching 在**每个 decode step** 检查：完成的请求立刻释放，新请求立刻插入。

```
Static Batching:
  Req A: ████████████████████░░░░░░░░  (已完成，但仍占 slot 等 B 结束)
  Req B: ████████████████████████████  (最长请求)
  Req C: ░░░░░░░░░░░░░░░░░░░░░░░░░░░░  (排队等待)
                                      ↑ 整个 batch 结束才接 Req C

Continuous Batching:
  Req A: ████████████████████
  Req B: ████████████████████████████
  Req C: ░░░░░░░░░░░░░░░░████████████  ← Req A 结束后立刻插入
```

核心收益：**极大提升 GPU 利用率和吞吐**。这是 vLLM 最基础的优化之一。

### 如何开启

**默认启用，无需配置。** vLLM 的调度器天然就是 continuous batching。

关键调度参数：

```bash
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --max-num-seqs 256              # 同时 batch 中最大请求数
    --max-num-batched-tokens 8192   # 每步最多处理的 token 数
```

## 权重激活量化

将模型权重从 FP16（2 bytes）压缩到 INT4/INT8（0.5/1 byte），**减少模型加载显存和推理时的内存带宽需求**。

```
Llama-3.1-70B 权重显存:
  FP16:  70B × 2 bytes ≈ 140 GB  → 需要 2× A100-80G
  INT4:  70B × 0.5 bytes ≈ 35 GB → 1× A100-80G 即可

Decode 阶段 (memory-bound):
  FP16: 读 140 GB 权重 / A100带宽 2TB/s ≈ 70ms per token
  INT4: 读 35 GB 权重 / 2TB/s ≈ 17.5ms per token  → 4x 带宽加速
```

常见量化方法：

| 方法 | 位宽 | 特点 |
|------|------|------|
| **AWQ** | W4A16 | 权重 4bit，激活 FP16，quality loss 小 |
| **GPTQ** | W4A16 | 需要校准数据集，与 AWQ 互为替代 |
| **FP8** | W8A8 | 权重和激活都 8bit，H100 原生支持，精度损失极小 |
| **BitsAndBytes** | W4/W8 | 简单易用，但推理速度不如 AWQ/GPTQ |

**WxAy 标记**：W = Weight 位宽，A = Activation 位宽。

BitsAndBytes Tim Dettmers 开发的量化库，在 HuggingFace 生态中非常流行，加载时自动完成，vLLM 从 v0.3.2 开始支持 BitsAndBytes，不建议 生产使用。 

```
W4A16 (AWQ/GPTQ) — 只量化权重，激活保持高精度:
  权重存储: FP16 → INT4 (离线量化，省 4x 显存)
  推理计算: INT4 权重 → dequant 到 FP16 → FP16 × FP16 矩阵乘
  收益: 省显存 + 省带宽（decode 阶段 memory-bound，读权重量减少 4x）
  代价: dequant 有额外开销，计算本身仍是 FP16 精度

W8A8 (FP8) — 权重和激活都量化，计算也在低精度执行:
  权重存储: FP16 → FP8 (省 2x 显存)
  激活量化: 推理时 FP16 激活 → 动态量化到 FP8（per-tensor 或 per-token scale）
  推理计算: FP8 × FP8 矩阵乘 → FP16 累加输出
            ↑ H100 FP8 Tensor Core 原生支持，算力翻倍
  收益: 省显存 + 省带宽 + 计算本身快 2x
  代价: 需要 H100/Ada 架构硬件

W4A8 — 权重压得更狠，激活适度量化:
  权重存储: FP16 → INT4 (省 4x 显存)
  激活量化: 推理时 FP16 激活 → 动态量化到 INT8/FP8
  推理计算: INT4 权重 → dequant 到 INT8 → INT8 × INT8 矩阵乘
  收益: 兼顾 W4 的极致显存压缩 + A8 的计算加速
  代价: 精度损失比 W4A16 略大（激活也损失了精度）
```

核心区别在于**激活是否量化**：
- A16：激活保持 FP16，矩阵乘在 FP16 精度，**只省存储不加速计算**
- A8：激活也量化，矩阵乘在 INT8/FP8 精度，**既省存储又加速计算**


不量化激活 (W4A16):
  权重: INT4 → dequant → FP16
  激活: 始终 FP16                     ← 精度高，但矩阵乘只能用 FP16 单元
  计算: FP16 × FP16 = FP16

量化激活 (W8A8):
  权重: FP8 (已存好)
  激活: FP16 → 动态量化 → FP8         ← 推理时实时压缩！
  计算: FP8 × FP8 = FP16             ← 可以用 FP8 Tensor Core，算力翻倍
        ↑
        矩阵乘的两个操作数都是低精度，硬件才能真正加速

1. 上一层输出 FP16 激活 y = [0.123, -0.456, 0.789, ...]
2. 计算 scale = max(|y|) / 127                          ← 找到动态范围
3. 量化:  y_int8 = round(y / scale) = [16, -58, 100, ...]  ← FP16 → INT8
4. 矩阵乘: z = W_int8 × y_int8                          ← 两边都是整数，快！
5. 反量化输出: z_fp16 = z × (scale_w × scale_y)          ← 恢复精度

本质： 
权重量化：压缩存储（静态的，离线做好）→ 省显存、省带宽
激活量化：压缩计算过程中的中间结果（动态的，每次推理实时做）→ 让硬件能用低精度算力加速矩阵乘



### 如何开启

```bash
# 直接加载已量化模型（HuggingFace 上大量 AWQ/GPTQ 模型）
vllm serve TheBloke/Llama-2-70B-Chat-AWQ \
    --quantization awq

# 在线 FP8 量化（H100/Ada Lovelace 架构）
vllm serve meta-llama/Llama-3.1-70B-Instruct \
    --quantization fp8
```
AWQ/GPTQ 适合消费级 GPU（RTX 4090 等），FP8 适合数据中心卡（H100/H200），选择取决于硬件。


# 核心调优参数

## 优化首 Token 延迟（TTFT）

Time To First Token — 用户发出请求到收到第一个 token 的耗时。**对交互体验影响最大。**

TTFT 主要由 prefill 阶段决定，优化思路是**减少 prefill 计算量**或**加速 prefill 执行**。

| 参数 | 作用 | 推荐值 |
|------|------|--------|
| `--enable-prefix-caching` | 复用前缀 KV Cache，跳过已缓存部分的 prefill | 默认开启 |
| `--enable-chunked-prefill` | 虽然会略增 TTFT，但防止长 prompt 饿死其他请求 | 默认开启 |
| `--max-model-len` | 限制最大序列长度，减少不必要的显存预留 | 按实际需求设置 |
| `--gpu-memory-utilization` | 提高到 0.95 可以为 KV Cache 留更多空间 | 0.90~0.95 |
| `--tensor-parallel-size` | 多卡并行切分 prefill 计算 | 按卡数设置 |

```bash
# 优化 TTFT 的典型配置
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --enable-prefix-caching \
    --gpu-memory-utilization 0.95 \
    --max-model-len 4096
```



## 提升吞吐（Throughput）

吞吐 = 单位时间处理的 token 总数（tokens/s）。**在线服务关注请求级吞吐**

核心思路：**让 GPU 尽可能忙碌，增大有效 batch size**。

| 参数 | 作用 | 推荐值 |
|------|------|--------|
| `--max-num-seqs` | 最大并发 batch 数 | 256（默认），显存允许可更高 |
| `--max-num-batched-tokens` | 每步最多处理 token 数 | 8192~32768 |
| `--gpu-memory-utilization` | 尽量利用显存给 KV Cache | 0.90~0.95 |
| `--tensor-parallel-size` | 多卡并行 | 视模型大小和卡数 |
| `--kv-cache-dtype fp8` | KV Cache 减半，batch 翻倍 | FP8 |
| `--quantization` | 权重压缩，省显存 | awq / fp8 |
| `--enable-prefix-caching` | 减少重复计算 | 默认开启 |
| `--disable-log-requests` | 减少日志 I/O 开销 | 高负载时开启 |

```bash
# 吞吐优先的典型配置
vllm serve meta-llama/Llama-3.1-70B-Instruct \
    --tensor-parallel-size 4 \
    --quantization fp8 \
    --kv-cache-dtype fp8 \
    --max-num-seqs 512 \
    --max-num-batched-tokens 16384 \
    --gpu-memory-utilization 0.95 \
    --enable-prefix-caching \
    --disable-log-requests
```

## 降低逐 Token 延迟（TPOT）

Time Per Output Token — decode 阶段每生成一个 token 的耗时。**影响用户感知的"打字速度"。**

| 参数 | 作用 | 推荐值 |
|------|------|--------|
| `--enable-chunked-prefill` | 防止大 prefill 阻塞 decode | 默认开启 |
| `--max-num-batched-tokens` | chunk 越小，decode 被打断越少 | 2048~4096（延迟优先） |
| `--max-num-seqs` | 降低可减少单步 decode 耗时 | 按需减小 |
| Speculative decoding | 用小模型猜测减少大模型 forward 次数 | `--num-speculative-tokens 5` |
| CUDA Graph | 消除 kernel launch overhead | 默认开启 |

```bash
# 延迟优先配置（在线聊天场景）
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --enable-chunked-prefill \
    --max-num-batched-tokens 2048 \
    --max-num-seqs 64 \
    --enable-prefix-caching
```

**TTFT vs TPOT vs Throughput 是三角权衡**：增大 batch 提升吞吐，但会增加 TPOT；chunked prefill 降低 TPOT 波动，但增加 TTFT。根据场景侧重调整。

