---
title: litellm核心特性
date: 2026-02-18 10:00:00 +0800
categories: [LLM, Inference]
tags: [litellm, gateway]
pin: true
---


# 核心特性

## 抽象适配 N Provider
OpenAI → Provider, 大量的插件来适配 proxy到不同的llm (OpenAI、Anthropic、Azure、Bedrock...). 
  BaseLLMHTTPHandler 中用httpx proxy过去。 
openai_compatible_providers 兼容的统一链路link 适配。 

本质 **OpenAI API 格式作为内部标准**。 

## Proxy 流水线

本质 LLM 世界的 JDBC。 

`base_process_llm_request` 内部的完整流水线：

```
1. user_api_key_auth()          → 认证鉴权（API Key → User/Team/Org 信息）
       │
2. Proxy Hooks (pre-call)       → max_budget_limiter + parallel_request_limiter
       │
3. Router.aroute_request()      → 负载均衡选择最佳 Deployment
       │
4. litellm.acompletion()        → 调用 SDK 层（Translation Layer）
       │
5. completion_cost()            → 成本计算（token × price）
       │
6. Response Headers             → x-litellm-response-cost, x-litellm-model-id
       │
7. _ProxyDBLogger               → 异步写入 PostgreSQL（批量刷新）
```

check_budget_limits check_rate_limits 限流。 User、Team、Organization，以及各自的预算上限和速率限制。

## 预算和成本控制

本质内存计算和异步刷新， 和我们之前做热点key思路类似。 

```
请求完成
    │
    ├─▶ completion_cost(model, usage)             # 即时计算
    │       cost = (prompt_tokens × input_price
    │             + completion_tokens × output_price) / 1_000_000
    │
    ├─▶ SpendUpdateQueue.add_update()             # 写入内存队列
    │       entity_type: KEY | USER | TEAM | ORG
    │       entity_id: hashed_token / user_id / team_id
    │       response_cost: float
    │
    ├─▶ DailySpendUpdateQueue.add_update()        # 日维度聚合
    │
    └─▶ RedisUpdateBuffer (可选, 高 RPS 场景)
            │
            ▼ 每 60 秒由后台任务触发
        PodLockManager.acquire_lock()             # 仅 Leader Pod 执行
            │
            ▼
        _commit_spend_updates_to_db()             # 批量 UPDATE , upinsert PostgreSQL
            Prisma batch_().update_many(
                spend: {increment: response_cost}
            )

{user_id}_{date}_{api_key}_{model}_{custom_llm_provider}_{endpoint}

请求完成 (startTime = 2026-02-22T14:30:00Z)
    │
    ├─ 截断日期 → "2026-02-22"
    │
    ├─ 生成 key → "H1_2026-02-22_sk-abc_openai/gpt-4o_openai_/chat/completions"
    │
    ├─ 写入内存 DailySpendUpdateQueue (按 key 聚合累加)
    │
    └─ 每 60 秒 flush ──▶ PostgreSQL UPSERT
                             │
                             ├─ 行不存在 → INSERT (spend=0.10, tokens=1600...)
                             └─ 行已存在 → UPDATE (spend += 0.10, tokens += 1600...)

hashed_token 是 API Key 的哈希值。 大致分级例子：
                        ┌──────────────────────────────────┐
                        │   Organization: "X Corp"       │
                        │   org_id: org_001                 │
                        │   max_budget: $10,000/月          │
                        └──────────┬───────────────────────┘
                                   │
                  ┌────────────────┼────────────────┐
                  ▼                                 ▼
      ┌─────────────────────┐          ┌──────────────────────┐
      │  Team: "A组"       │          │  Team: "B组"        │
      │  team_id: team_be    │          │  team_id: team_data   │
      │  max_budget: $3,000  │          │  max_budget: $5,000   │
      └─────────┬───────────┘          └────────────────────────┘
                │
       ┌────────┼────────┐
       ▼                 ▼
  ┌──────────┐     ┌──────────┐
  │ User: H1 │     │ User: H2 │
  │ user_id:  │     │ user_id:  │
  │ zhang3    │     │ li4       │
  │ budget:   │     │ budget:   │
  │ $500      │     │ $800      │
  └─────┬─────┘     └───────────┘
        │
   ┌────┼────────────┐
   ▼                  ▼
┌────────────┐  ┌────────────┐
│ Token A     │  │ Token B     │
│ sk-abc...   │  │ sk-def...   │
│ hashed:     │  │ hashed:     │
│ 7f3a2b...   │  │ 9c1d4e...   │
│ budget: $200│  │ budget: $100│
│ 用途: VoiceAgent│  │ 用途: SreAgent   │
└─────────────┘  └─────────────┘

"Org 本月已超 $10,000"

请求进来 → 检查 Token 预算 ✓ → 检查 User 预算 ✓ → 检查 Team 预算 ✓ → 检查 Org 预算 ✗ → 拒绝！
```

                                                                      
## Router：智能负载均衡

**Deployment**——同一个逻辑模型可以有多个物理部署。Router 从N个 Deployment 中根据策略选择一个。

本质是基于metrics 根据策略选1个。 

当某个 Deployment 连续失败时，Router 会将其暂时"冷却" CoolDown，本质就是 熔断器。 

### 路由策略
```python
# router_strategy/ — 8 种策略

# 1. simple-shuffle: 随机打乱（默认）
class SimpleShuffleStrategy:
    """均匀分布流量"""

# 2. lowest-latency: 最低延迟
class LowestLatencyLoggingHandler(CustomLogger):
    """追踪每个 Deployment 的延迟，选最快的"""
    def log_success_event(self, kwargs, response_obj, start_time, end_time):
        latency = end_time - start_time
        deployment_id = kwargs["litellm_params"]["model_info"]["id"]
        # 记录到 DualCache，维护滑动窗口

    def get_available_deployments(self, model, healthy_deployments):
        # 按平均延迟排序返回

# 3. lowest-cost: 最低成本
class LowestCostLoggingHandler(CustomLogger):
    """按模型价格排序"""

# 4. least-busy: 最空闲（基于 TPM/RPM）
class LeastBusyLoggingHandler(CustomLogger):
    """追踪每个 Deployment 当前的 TPM/RPM，选最空闲的"""

# 6. lowest-tpm-rpm-v2: TPM/RPM 感知（v2，多实例安全）
class LowestTPMLoggingHandler_v2(BaseRoutingStrategy, CustomLogger):
"""缓存粒度从 model_group → 单个 deployment，
    使用 Redis INCR 原子操作 + batch_get（mget）批量读取，
    支持多 Pod 跨实例的精确限流"""

# 7. budget-limiter: 预算感知路由（过滤器，可叠加其他策略）
class RouterBudgetLimiting(CustomLogger):
    """三级预算过滤：Provider 级 / Deployment 级 / Tag 级
    配置示例：openai: {budget_limit: 100, time_period: 1d}
    超预算的 Deployment 直接过滤掉,仅返回预算内的
    """

# 8. tag-based-routing: 标签路由（过滤器）
async def get_deployments_for_tag(llm_router, healthy_deployments, request_kwargs):

    """请求 metadata.tags 与 Deployment litellm_params.tags 匹配
    匹配模式：match_any=True（交集非空即可）/ match_any=False（子集匹配）
    未匹配到 → 回退到标记了 'default' 标签的 Deployment
    全部无标签 → 返回所有 healthy_deployments"""

```


TPM = Tokens Per Minute，每分钟消耗的 token 总数（prompt_tokens + completion_tokens）。
RPM = Requests Per Minute，每分钟请求次数。

本质是 redis counter，集中metrics. 


14:30:05  请求A → OpenAI 成功，用了 500 tokens
          → TPM: {openai_deploy: 500, azure_deploy: 0}
          → RPM: {openai_deploy: 1,   azure_deploy: 0}

14:30:12  请求B 进来，Router 读取 TPM dict
          → openai_deploy=500, azure_deploy=0
          → 选 azure_deploy（TPM 最低）

14:30:12  请求B → Azure 成功，用了 800 tokens
          → TPM: {openai_deploy: 500, azure_deploy: 800}
          → RPM: {openai_deploy: 1,   azure_deploy: 1}

14:30:20  请求C 进来
          → openai_deploy=500 < azure_deploy=800
          → 选 openai_deploy

14:31:00  分钟窗口切换到 "14-31"
          → 旧 key "gpt-4:tpm:14-30" 在 60 秒后 TTL 过期
          → 所有 Deployment 从 0 开始重新计算

DualCache 本质上是 2级 mem+redis，和之前 计数器思路类似。
 读：L1 → L2 → 回填 L1
 写：同时写 L1 + L2
本质是 缓解 read 压力。 

### Fallback

失败 → 标记 cooldown → 继续下一个

Fastapi 响应头 `x-litellm-attempted-deployments` 会记录尝试过的所有 Deployment.


## 可观测性

各种插件 集成  "langfuse", "prometheus", "datadog" 等等。 


## 成本计算

cost_calculator.py 每个 Provider 的 cost_per_token 函数。 
- **OpenAI**：按 prompt_tokens × input_price + completion_tokens × output_price
- **Anthropic**：增加了 cache_creation_input_price 和 cache_read_input_price
- **Bedrock**：按区域定价，需要查询 AWS Pricing API 或本地映射表


```python
# cost_calculator.py 核心公式
def completion_cost(model, usage, custom_llm_provider=None):
    """
    cost = (prompt_tokens × input_cost_per_token
          + completion_tokens × output_cost_per_token)

    特殊处理：
    - cache_creation_input_tokens × cache_creation_input_price
    - cache_read_input_tokens × cache_read_input_price
    - audio_tokens × audio_token_price
    - custom_cost_per_token 覆盖
    - add_margin_to_model_cost 利润率
    """
```
逻辑是通用的，数据是特化的。
定价数据在 model_prices_and_context_window.json中。 





